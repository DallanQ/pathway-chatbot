# The Llama Cloud API key.
# LLAMA_CLOUD_API_KEY=

# The provider for the AI models to use.
MODEL_PROVIDER=openai

# The name of LLM model to use.
MODEL=gpt-4o-mini

# Name of the embedding model to use.
EMBEDDING_MODEL=text-embedding-3-large

# Dimension of the embedding model to use.
EMBEDDING_DIM=3072

# The questions to help users get started (multi-line).
# CONVERSATION_STARTERS=

# The OpenAI API key to use.
OPENAI_API_KEY=your openai key

# Temperature for sampling from the model.
# LLM_TEMPERATURE=0

# Maximum number of tokens to generate.
# LLM_MAX_TOKENS=

# The number of similar embeddings to return when retrieving documents.
TOP_K=3

# The time in milliseconds to wait for the stream to return a response.
STREAM_TIMEOUT=60000

# Configuration for Pinecone vector store
# The Pinecone API key.
PINECONE_API_KEY=your pinecone api key
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=pathway
 
# Voyage
VOYAGE_API_KEY=your voyage api key

# FILESERVER_URL_PREFIX is the URL prefix of the server storing the images generated by the interpreter.
FILESERVER_URL_PREFIX=http://localhost:8000/api/files

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# The system prompt for the AI model.
SYSTEM_PROMPT=You are a helpful assistant who helps users with their questions.

# Monitoring tool
# Langfuse
LANGFUSE_SECRET_KEY=your langfuse secret key
LANGFUSE_PUBLIC_KEY=your langfuse public key
LANGFUSE_HOST=https://us.cloud.langfuse.com

# Geoapify
GEOAPIFY_API_KEY=your_geoapify_api_key

# ============================================================================
# MEMORY MONITORING & DIAGNOSTICS CONFIGURATION
# ============================================================================

# Master switch for ALL memory monitoring features
# Set to 'false' to completely disable memory diagnostics and save resources
# This disables: tracemalloc, object counting, snapshots, malloc_trim, leak reports
ENABLE_MEMORY_MONITORING=true

# Number of stack frames to capture in tracemalloc (default: 25)
# Higher values provide more detail but use more memory
TRACEMALLOC_FRAMES=25

# Interval between memory snapshots in seconds (default: 300 = 5 minutes)
SNAPSHOT_INTERVAL_SECONDS=300

# Interval for malloc_trim to return memory to OS (default: 300 = 5 minutes)
MALLOC_TRIM_INTERVAL_SECONDS=300

# Interval for leak analysis reports in minutes (default: 5 minutes)
# Set to higher values (e.g., 30 or 60) for less frequent detailed reports
MONITORING_LEAK_ANALYSIS_INTERVAL_MINUTES=5

# ============================================================================
# GENERAL MONITORING & S3 UPLOAD CONFIGURATION
# ============================================================================

# Enable monitoring S3 uploads (set to 'true' to enable)
ENABLE_MONITORING_S3_UPLOAD=true

# Enable heartbeat file uploads during idle periods (set to 'true' to enable)
# When enabled, uploads small JSON heartbeat files every 5 minutes even when no requests
# This proves the service is alive and monitoring is working
ENABLE_MONITORING_HEARTBEAT=true

# AWS S3 Configuration for monitoring reports
# AWS_ACCESS_KEY_ID=your_aws_access_key_id
# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
# AWS_REGION=us-east-1
# MONITORING_S3_BUCKET=your-monitoring-bucket-name
# MONITORING_S3_PREFIX=pathway-chatbot-backend-monitoring

# Monitoring intervals (in minutes)
# MONITORING_REPORT_INTERVAL_MINUTES=5
# MONITORING_GC_INTERVAL_MINUTES=10
# MONITORING_MEMORY_LOG_INTERVAL_MINUTES=60

# Emergency memory threshold (percentage of system memory)
# EMERGENCY_MEMORY_THRESHOLD_PERCENT=90.0
# EMERGENCY_ALERT_COOLDOWN_SECONDS=300

# Maximum metrics buffer size before auto-flush
# MAX_METRICS_BUFFER=500

# Local report cleanup (days to retain)
# CLEANUP_RETENTION_DAYS=7
