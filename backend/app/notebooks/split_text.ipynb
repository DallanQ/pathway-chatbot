{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for splitting markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_headers(par: str, header_levels: dict) -> dict:\n",
    "    \"\"\" Set headers for a paragraph \"\"\"\n",
    "    \n",
    "    # Check if paragraph starts with a header (e.g. '# ', '## ', etc.)\n",
    "    for level in range(1, 7):\n",
    "        if par.startswith('#' * level + ' '):\n",
    "            header_levels[level] = par\n",
    "            # Reset lower-level headers\n",
    "            for lower_level in range(level + 1, 7):\n",
    "                header_levels[lower_level] = None\n",
    "            # Exit loop after finding the correct header level\n",
    "            break  \n",
    "    \n",
    "    # Build headers dictionary dynamically\n",
    "    headers = {f'header_{i}': header_levels[i] for i in range(1, 7) if header_levels[i]}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_complet_context(par: str, header_levels: dict) -> dict:\n",
    "    \"\"\" Set the complete context for a paragraph \"\"\"\n",
    "\n",
    "    complet_context = ''\n",
    "    for header in header_levels:\n",
    "        complet_context += header_levels[header]\n",
    "        complet_context += '\\n\\n'\n",
    "\n",
    "    if par not in header_levels.values():\n",
    "        complet_context += par\n",
    "    \n",
    "    return complet_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "def split_document_text(paragraphs: list[str], split_by_sentence: bool = False) -> list[TextNode]:\n",
    "    \"\"\" Split text into paragraphs \"\"\"\n",
    "    result = []\n",
    "    headers = {}\n",
    "    header_levels = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}\n",
    "    \n",
    "    for par in paragraphs:\n",
    "        headers = set_headers(par, header_levels)\n",
    "        complet_context = set_complet_context(par, headers)\n",
    "        \n",
    "        metadata = {\n",
    "            'headers': headers,\n",
    "            'paragraph': par,\n",
    "            'complet_context': complet_context\n",
    "        }\n",
    "\n",
    "        # use spacy to split paragraph into sentences\n",
    "        if split_by_sentence:\n",
    "            doc = nlp(par)\n",
    "            for sent in doc.sents:\n",
    "                node = TextNode(metadata=metadata, text=sent.text)\n",
    "                result.append(node)\n",
    "\n",
    "        else:\n",
    "            # Create a TextNode and add to result\n",
    "            node = TextNode(metadata=metadata, text=par)\n",
    "            result.append(node)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "directory = 'data1/'\n",
    "\n",
    "# List to store all the results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.md'):  # Only process markdown files\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            #extract the heading subheading and title\n",
    "            \n",
    "            paragraphs = f.read().split('\\n')\n",
    "            \n",
    "            # Use spacy to split paragraphs into sentences\n",
    "            result = split_document_text(paragraphs, split_by_sentence=True)\n",
    "            \n",
    "            # Append results to the overall list\n",
    "            all_results.extend(result)\n",
    "             # Or just split into paragraphs\n",
    "             # result = split_document_text(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='0241f58c-1379-4deb-8cfe-36cd4cfe9c5f', embedding=None, metadata={'paragraph': '* Be a member of The Church of Jesus Christ of Latter-day Saints. For applicants who are not Church members, see [2.2.2 Non-Member Applicants](https://pathwaysupport.org/handbook/2-admission-and-tuition/admission/#non-member).', 'complet_context': '# 2.2 Admission Requirements\\n\\n* Be a member of The Church of Jesus Christ of Latter-day Saints. For applicants who are not Church members, see [2.2.2 Non-Member Applicants](https://pathwaysupport.org/handbook/2-admission-and-tuition/admission/#non-member).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For applicants who are not Church members, see [2.2.2 Non-Member Applicants](https://pathwaysupport.org/handbook/2-admission-and-tuition/admission/#non-member).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "_=load_dotenv(find_dotenv())\n",
    "# index = VectorStoreIndex(all_results)\n",
    "pc = Pinecone()\n",
    "pc_index = pc.Index(os.getenv(\"PINECONE_INDEX_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=PineconeVectorStore(pinecone_index=pc_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a54ebf2a154fbf90bd1c5814da6d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(nodes=all_results, storage_context=storage_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-0vVj5Jes-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
